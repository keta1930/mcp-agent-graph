import mag
import json


def get_available_models():
    """è·å–ç³»ç»Ÿä¸­å¯ç”¨çš„æ¨¡å‹åˆ—è¡¨"""
    try:
        models = mag.list_model()
        return [model['name'] for model in models if 'name' in model]
    except Exception as e:
        print(f"âŒ è·å–æ¨¡å‹åˆ—è¡¨å¤±è´¥: {e}")
        return []


def select_model(available_models):
    """è®©ç”¨æˆ·é€‰æ‹©æ¨¡å‹"""
    if not available_models:
        print("âŒ ç³»ç»Ÿä¸­æ²¡æœ‰å¯ç”¨çš„æ¨¡å‹ï¼Œè¯·å…ˆæ·»åŠ æ¨¡å‹é…ç½®")
        return None

    print("\nğŸ“‹ å¯ç”¨æ¨¡å‹åˆ—è¡¨:")
    for i, model in enumerate(available_models, 1):
        print(f"  {i}. {model}")

    while True:
        try:
            choice = input(f"\nğŸ”¤ è¯·é€‰æ‹©æ¨¡å‹ (1-{len(available_models)}): ").strip()
            if choice.isdigit():
                index = int(choice) - 1
                if 0 <= index < len(available_models):
                    selected_model = available_models[index]
                    print(f"âœ… å·²é€‰æ‹©æ¨¡å‹: {selected_model}")
                    return selected_model
            print(f"âš ï¸ è¯·è¾“å…¥ 1 åˆ° {len(available_models)} ä¹‹é—´çš„æ•°å­—")
        except KeyboardInterrupt:
            print("\nğŸ‘‹ æ“ä½œå·²å–æ¶ˆ")
            return None


def stream_generation(requirement, conversation_id, model):
    """æµå¼ç”ŸæˆMCPå·¥å…·"""
    print("\nğŸ”„ å¼€å§‹æµå¼ç”Ÿæˆ...\n")

    try:
        stream = mag.mcp_gen(
            requirement=requirement,
            model=model,
            conversation_id=conversation_id,
            user_id="demo_user",
            stream=True
        )

        for chunk in stream:
            # å¤„ç†OpenAIæ ¼å¼çš„chunk
            if "choices" in chunk and chunk["choices"]:
                delta = chunk["choices"][0].get("delta", {})

                # è¾“å‡ºç”Ÿæˆçš„å†…å®¹
                if delta.get("content"):
                    print(delta["content"], end="", flush=True)

                # è¾“å‡ºæ€è€ƒå†…å®¹ï¼ˆå¦‚æœæœ‰ï¼‰
                if delta.get("reasoning_content"):
                    print(f"\n[æ€è€ƒ]: {delta['reasoning_content']}", flush=True)

            # å¤„ç†é”™è¯¯ä¿¡æ¯
            elif "error" in chunk:
                print(f"\nâŒ é”™è¯¯: {chunk['error']['message']}")
                break

            # å¤„ç†å®Œæˆä¿¡æ¯
            elif "completion" in chunk:
                print(f"\n\nâœ… MCPå·¥å…·ç”Ÿæˆå®Œæˆ!")
                print(f"å·¥å…·åç§°: {chunk['completion']['tool_name']}")
                print(f"å®Œæˆæ¶ˆæ¯: {chunk['completion']['message']}")
                return True

            # å¤„ç†æœªå®Œæˆä¿¡æ¯
            elif "incomplete" in chunk:
                print(f"\n\nâš ï¸ MCPå·¥å…·ç”Ÿæˆæœªå®Œæˆ")
                print(f"æ¶ˆæ¯: {chunk['incomplete']['message']}")
                print(f"ç¼ºå°‘å­—æ®µ: {chunk['incomplete']['missing_fields']}")
                return False

        print("\n\næµå¼ç”Ÿæˆç»“æŸ")
        return False

    except Exception as e:
        print(f"\nâŒ æµå¼ç”Ÿæˆå¤±è´¥: {e}")
        return False


def nonstream_generation(requirement, conversation_id, model):
    """éæµå¼ç”ŸæˆMCPå·¥å…·"""
    print("\nâ³ å¼€å§‹éæµå¼ç”Ÿæˆ...")

    try:
        result = mag.mcp_gen(
            requirement=requirement,
            model=model,
            conversation_id=conversation_id,
            user_id="demo_user",
            stream=False
        )

        print("\nğŸ“‹ ç”Ÿæˆç»“æœ:")
        print(json.dumps(result, ensure_ascii=False, indent=2))

        # åˆ†æç”Ÿæˆç»“æœ
        if "completion" in result:
            print(f"\nâœ… MCPå·¥å…·ç”Ÿæˆå®Œæˆ!")
            print(f"å·¥å…·åç§°: {result['completion'].get('tool_name', 'æœªçŸ¥')}")
            print(f"å®Œæˆæ¶ˆæ¯: {result['completion'].get('message', 'æ— æ¶ˆæ¯')}")
            return True
        elif "incomplete" in result:
            print(f"\nâš ï¸ MCPå·¥å…·ç”Ÿæˆæœªå®Œæˆ")
            print(f"æ¶ˆæ¯: {result['incomplete'].get('message', 'æ— æ¶ˆæ¯')}")
            print(f"ç¼ºå°‘å­—æ®µ: {result['incomplete'].get('missing_fields', [])}")
            return False
        elif "errors" in result:
            print(f"\nâŒ ç”Ÿæˆè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯:")
            for error in result["errors"]:
                print(f"  - {error.get('message', 'æœªçŸ¥é”™è¯¯')}")
            return False
        else:
            print(f"\nğŸ“ ç”Ÿæˆè¿›è¡Œä¸­ï¼Œå…± {result.get('total_chunks', 0)} ä¸ªæ•°æ®å—")
            return False

    except Exception as e:
        print(f"\nâŒ éæµå¼ç”Ÿæˆå¤±è´¥: {e}")
        return False


def interactive_mcp_generation():
    """äº¤äº’å¼MCPç”Ÿæˆ"""
    print("ğŸ¤– AIç”ŸæˆMCPå·¥å…· - äº¤äº’å¼æ¨¡å¼")
    print("=" * 50)

    # è·å–å¯ç”¨æ¨¡å‹å¹¶è®©ç”¨æˆ·é€‰æ‹©
    print("ğŸ” æ­£åœ¨è·å–å¯ç”¨æ¨¡å‹...")
    available_models = get_available_models()
    current_model = select_model(available_models)

    if not current_model:
        print("âŒ æ— æ³•ç»§ç»­ï¼Œç¨‹åºé€€å‡º")
        return

    # è®©ç”¨æˆ·è¾“å…¥conversation_id
    while True:
        conversation_id = input("ğŸ“ è¯·è¾“å…¥å¯¹è¯ID (ä¾‹å¦‚: weather_tool_v1, file_manager, my_mcp_tool): ").strip()
        if conversation_id:
            break
        print("âš ï¸ å¯¹è¯IDä¸èƒ½ä¸ºç©ºï¼Œè¯·é‡æ–°è¾“å…¥")

    print(f"\nğŸ“ å¯¹è¯ID: {conversation_id}")
    print(f"ğŸ¤– å½“å‰æ¨¡å‹: {current_model}")
    print("\nğŸ’¡ ä½¿ç”¨è¯´æ˜:")
    print("- è¾“å…¥ä½ çš„MCPå·¥å…·éœ€æ±‚")
    print("- è¾“å…¥ 'stream' åˆ‡æ¢åˆ°æµå¼æ¨¡å¼")
    print("- è¾“å…¥ 'nonstream' åˆ‡æ¢åˆ°éæµå¼æ¨¡å¼")
    print("- è¾“å…¥ 'model' åˆ‡æ¢æ¨¡å‹")
    print("- è¾“å…¥ '<end>END</end>' å®Œæˆå·¥å…·ç”Ÿæˆ")
    print("- è¾“å…¥ 'quit' é€€å‡ºç¨‹åº")

    # é»˜è®¤ä½¿ç”¨æµå¼æ¨¡å¼
    use_stream = True
    mode_text = "æµå¼" if use_stream else "éæµå¼"
    print(f"\nğŸ”§ å½“å‰æ¨¡å¼: {mode_text}")

    while True:
        print("\n" + "-" * 30)
        requirement = input("ğŸ”¤ è¯·è¾“å…¥éœ€æ±‚ (æˆ–å‘½ä»¤): ").strip()

        if not requirement:
            print("âš ï¸ è¾“å…¥ä¸èƒ½ä¸ºç©ºï¼Œè¯·é‡æ–°è¾“å…¥")
            continue

        # å¤„ç†é€€å‡ºå‘½ä»¤
        if requirement.lower() == 'quit':
            print("ğŸ‘‹ å†è§ï¼")
            break

        # å¤„ç†æ¨¡å¼åˆ‡æ¢å‘½ä»¤
        if requirement.lower() == 'stream':
            use_stream = True
            print("âœ… å·²åˆ‡æ¢åˆ°æµå¼æ¨¡å¼")
            continue
        elif requirement.lower() == 'nonstream':
            use_stream = False
            print("âœ… å·²åˆ‡æ¢åˆ°éæµå¼æ¨¡å¼")
            continue

        # å¤„ç†æ¨¡å‹åˆ‡æ¢å‘½ä»¤
        if requirement.lower() == 'model':
            print("ğŸ” é‡æ–°è·å–å¯ç”¨æ¨¡å‹...")
            available_models = get_available_models()
            new_model = select_model(available_models)
            if new_model and new_model != current_model:
                current_model = new_model
                print(f"âœ… æ¨¡å‹å·²åˆ‡æ¢åˆ°: {current_model}")
            continue

        # æ‰§è¡Œç”Ÿæˆ
        if use_stream:
            completed = stream_generation(requirement, conversation_id, current_model)
        else:
            completed = nonstream_generation(requirement, conversation_id, current_model)

        # å¦‚æœå®Œæˆäº†å·¥å…·ç”Ÿæˆï¼Œè¯¢é—®æ˜¯å¦ç»§ç»­
        if completed:
            continue_choice = input("\nğŸ”„ å·¥å…·å·²å®Œæˆï¼Œæ˜¯å¦ç»§ç»­ä¼˜åŒ–ï¼Ÿ(y/n): ").strip().lower()
            if continue_choice != 'y':
                print("ğŸ‰ MCPå·¥å…·ç”Ÿæˆå®Œæˆï¼")
                break


if __name__ == "__main__":
    interactive_mcp_generation()