<div class="max-w-4xl">
    <div class="mb-8">
        <h1 class="text-3xl font-bold text-gray-900 mb-4">Model Management</h1>
        <p class="text-lg text-gray-600">Configure and manage AI models for your graphs. MAG SDK supports multiple model providers and allows flexible model configuration for different use cases.</p>
    </div>

    <!-- Understanding Models in MAG ---->
    <div class="mb-8">
        <h2 class="text-2xl font-semibold text-gray-900 mb-4">Understanding Models in MAG</h2>
        <div class="bg-blue-50 border-l-4 border-blue-400 p-4 mb-6">
            <div class="flex">
                <div class="flex-shrink-0">
                    <i class="fas fa-info-circle text-blue-400"></i>
                </div>
                <div class="ml-3">
                    <p class="text-sm text-blue-700">
                        MAG SDK supports multiple AI model providers including <strong>OpenAI</strong>, <strong>Anthropic</strong>, <strong>Google</strong>, and <strong>local models</strong>. You can configure different models for different graphs or even different nodes within the same graph.
                    </p>
                </div>
            </div>
        </div>

        <div class="grid grid-cols-1 lg:grid-cols-3 gap-4 mb-6">
            <div class="bg-white border border-gray-200 rounded-lg p-4">
                <h3 class="font-semibold text-gray-900 mb-2">
                    <i class="fas fa-cloud text-blue-600 mr-2"></i>
                    Cloud Providers
                </h3>
                <p class="text-sm text-gray-600">OpenAI GPT, Anthropic Claude, Google Gemini, and other cloud-based models</p>
            </div>
            <div class="bg-white border border-gray-200 rounded-lg p-4">
                <h3 class="font-semibold text-gray-900 mb-2">
                    <i class="fas fa-server text-green-600 mr-2"></i>
                    Local Models
                </h3>
                <p class="text-sm text-gray-600">Run models locally using Ollama, LM Studio, or custom inference servers</p>
            </div>
            <div class="bg-white border border-gray-200 rounded-lg p-4">
                <h3 class="font-semibold text-gray-900 mb-2">
                    <i class="fas fa-cog text-purple-600 mr-2"></i>
                    Flexible Configuration
                </h3>
                <p class="text-sm text-gray-600">Different models for different tasks, custom parameters, and fallback options</p>
            </div>
        </div>
    </div>

    <!-- Listing Available Models ---->
    <div class="mb-8">
        <h2 class="text-2xl font-semibold text-gray-900 mb-4">
            <i class="fas fa-list text-green-600 mr-2"></i>
            Listing Available Models
        </h2>
        <p class="text-gray-600 mb-4">Check which models are currently configured and available:</p>

        <div class="bg-gray-50 rounded-lg p-4 border relative group mb-6">
            <button class="absolute top-2 right-2 p-2 text-gray-500 hover:text-gray-700 opacity-0 group-hover:opacity-100 transition-opacity" onclick="copyToClipboard('# List all available models\nmodels = mag.list_models()\n\nprint(f\"Available models: {len(models)}\")\nfor model in models:\n    name = model.get(\"name\")\n    provider = model.get(\"provider\")\n    model_id = model.get(\"model_id\")\n    status = \"✅ Active\" if model.get(\"active\") else \"⏸️ Inactive\"\n    \n    print(f\"\\n{name} ({provider}) - {status}\")\n    print(f\"  Model ID: {model_id}\")\n    if model.get(\"description\"):\n        print(f\"  Description: {model.get(\\'description\\')}\") \n    \n    # Show configuration details\n    config = model.get(\"config\", {})\n    if config:\n        print(\"  Configuration:\")\n        for key, value in config.items():\n            print(f\"    {key}: {value}\")')">
                <i class="fas fa-copy"></i>
            </button>
            <pre><code class="language-python"># List all available models
import mag

models = mag.list_model()

print(f"Available models: {len(models)}")
for model in models:
    name = model.get("name")
    provider = model.get("provider")
    model_id = model.get("model_id")
    status = "✅ Active" if model.get("active") else "⏸️ Inactive"

    print(f"\n{name} ({provider}) - {status}")
    print(f"  Model ID: {model_id}")
    if model.get("description"):
        print(f"  Description: {model.get('description')}")

    # Show configuration details
    config = model.get("config", {})
    if config:
        print("  Configuration:")
        for key, value in config.items():
            print(f"    {key}: {value}")</code></pre>
        </div>
    </div>

    <!-- Adding New Models ---->
    <div class="mb-8">
        <h2 class="text-2xl font-semibold text-gray-900 mb-4">
            <i class="fas fa-plus text-blue-600 mr-2"></i>
            Adding New Models
        </h2>
        <p class="text-gray-600 mb-4">Register new models from different providers:</p>

        <!-- OpenAI Model ---->
        <div class="mb-6">
            <h3 class="text-lg font-semibold text-gray-900 mb-3">
                <i class="fas fa-robot text-green-500 mr-2"></i>
                OpenAI Models
            </h3>
            <div class="bg-gray-50 rounded-lg p-4 border relative group">
                <button class="absolute top-2 right-2 p-2 text-gray-500 hover:text-gray-700 opacity-0 group-hover:opacity-100 transition-opacity" onclick="copyToClipboard('# Add an OpenAI model\nresult = mag.add_model(\n    name=\"gpt4-turbo\",\n    provider=\"openai\",\n    model_id=\"gpt-4-1106-preview\",\n    config={\n        \"api_key\": \"your-openai-api-key\",  # Or use environment variable\n        \"temperature\": 0.7,\n        \"max_tokens\": 4000,\n        \"top_p\": 0.9\n    },\n    description=\"GPT-4 Turbo for complex reasoning tasks\"\n)\n\nif result.get(\"success\"):\n    print(f\"Added model: {result.get(\\'name\\')}\") \nelse:\n    print(f\"Failed to add model: {result.get(\\'error\\')}\") \n\n# Set as default model (optional)\nmag.set_default_model(\"gpt4-turbo\")')">
                    <i class="fas fa-copy"></i>
                </button>
                <pre><code class="language-python"># Add an OpenAI model
result = mag.add_model(
    name="gpt4-turbo",
    provider="openai",
    model_id="gpt-4-1106-preview",
    config={
        "api_key": "your-openai-api-key",  # Or use environment variable
        "temperature": 0.7,
        "max_tokens": 4000,
        "top_p": 0.9
    },
    description="GPT-4 Turbo for complex reasoning tasks"
)

if result.get("success"):
    print(f"Added model: {result.get('name')}")
else:
    print(f"Failed to add model: {result.get('error')}")

# Set as default model (optional)
mag.set_default_model("gpt4-turbo")</code></pre>
            </div>
        </div>

        <!-- Anthropic Model ---->
        <div class="mb-6">
            <h3 class="text-lg font-semibold text-gray-900 mb-3">
                <i class="fas fa-brain text-purple-500 mr-2"></i>
                Anthropic Models
            </h3>
            <div class="bg-gray-50 rounded-lg p-4 border relative group">
                <button class="absolute top-2 right-2 p-2 text-gray-500 hover:text-gray-700 opacity-0 group-hover:opacity-100 transition-opacity" onclick="copyToClipboard('# Add an Anthropic Claude model\nresult = mag.add_model(\n    name=\"claude-3-5-sonnet\",\n    provider=\"anthropic\",\n    model_id=\"claude-3-5-sonnet-20241022\",\n    config={\n        \"api_key\": \"your-anthropic-api-key\",\n        \"max_tokens\": 8192,\n        \"temperature\": 0.3\n    },\n    description=\"Claude 3.5 Sonnet for analytical tasks\"\n)\n\n# Add Claude Haiku for faster responses\nhaiku_result = mag.add_model(\n    name=\"claude-haiku\",\n    provider=\"anthropic\", \n    model_id=\"claude-3-haiku-20240307\",\n    config={\n        \"api_key\": \"your-anthropic-api-key\",\n        \"max_tokens\": 4096,\n        \"temperature\": 0.5\n    },\n    description=\"Claude Haiku for quick responses\"\n)')">
                    <i class="fas fa-copy"></i>
                </button>
                <pre><code class="language-python"># Add an Anthropic Claude model
result = mag.add_model(
    name="claude-3-5-sonnet",
    provider="anthropic",
    model_id="claude-3-5-sonnet-20241022",
    config={
        "api_key": "your-anthropic-api-key",
        "max_tokens": 8192,
        "temperature": 0.3
    },
    description="Claude 3.5 Sonnet for analytical tasks"
)

# Add Claude Haiku for faster responses
haiku_result = mag.add_model(
    name="claude-haiku",
    provider="anthropic",
    model_id="claude-3-haiku-20240307",
    config={
        "api_key": "your-anthropic-api-key",
        "max_tokens": 4096,
        "temperature": 0.5
    },
    description="Claude Haiku for quick responses"
)</code></pre>
            </div>
        </div>

        <!-- Local Model (Ollama) ---->
        <div class="mb-6">
            <h3 class="text-lg font-semibold text-gray-900 mb-3">
                <i class="fas fa-home text-orange-500 mr-2"></i>
                Local Models (Ollama)
            </h3>
            <div class="bg-gray-50 rounded-lg p-4 border relative group">
                <button class="absolute top-2 right-2 p-2 text-gray-500 hover:text-gray-700 opacity-0 group-hover:opacity-100 transition-opacity" onclick="copyToClipboard('# Add a local Ollama model\nresult = mag.add_model(\n    name=\"llama3-local\",\n    provider=\"ollama\",\n    model_id=\"llama3:8b\",\n    config={\n        \"base_url\": \"http://localhost:11434\",  # Default Ollama endpoint\n        \"temperature\": 0.8,\n        \"num_predict\": 2048\n    },\n    description=\"Local Llama 3 8B model via Ollama\"\n)\n\n# Add a coding-specific local model\ncoding_result = mag.add_model(\n    name=\"codellama-local\",\n    provider=\"ollama\",\n    model_id=\"codellama:13b\",\n    config={\n        \"base_url\": \"http://localhost:11434\",\n        \"temperature\": 0.2,  # Lower temperature for code generation\n        \"num_predict\": 4096\n    },\n    description=\"Local CodeLlama for coding tasks\"\n)')">
                    <i class="fas fa-copy"></i>
                </button>
                <pre><code class="language-python"># Add a local Ollama model
result = mag.add_model(
    name="llama3-local",
    provider="ollama",
    model_id="llama3:8b",
    config={
        "base_url": "http://localhost:11434",  # Default Ollama endpoint
        "temperature": 0.8,
        "num_predict": 2048
    },
    description="Local Llama 3 8B model via Ollama"
)

# Add a coding-specific local model
coding_result = mag.add_model(
    name="codellama-local",
    provider="ollama",
    model_id="codellama:13b",
    config={
        "base_url": "http://localhost:11434",
        "temperature": 0.2,  # Lower temperature for code generation
        "num_predict": 4096
    },
    description="Local CodeLlama for coding tasks"
)</code></pre>
            </div>
        </div>
    </div>

    <!-- Model Configuration ---->
    <div class="mb-8">
        <h2 class="text-2xl font-semibold text-gray-900 mb-4">
            <i class="fas fa-cog text-purple-600 mr-2"></i>
            Model Configuration Parameters
        </h2>
        <p class="text-gray-600 mb-4">Understanding common configuration parameters for different providers:</p>

        <div class="overflow-x-auto mb-6">
            <table class="min-w-full bg-white border border-gray-200 rounded-lg">
                <thead class="bg-gray-50">
                    <tr>
                        <th class="px-6 py-3 text-left text-xs font-medium text-gray-500 uppercase tracking-wider">Parameter</th>
                        <th class="px-6 py-3 text-left text-xs font-medium text-gray-500 uppercase tracking-wider">OpenAI</th>
                        <th class="px-6 py-3 text-left text-xs font-medium text-gray-500 uppercase tracking-wider">Anthropic</th>
                        <th class="px-6 py-3 text-left text-xs font-medium text-gray-500 uppercase tracking-wider">Ollama</th>
                        <th class="px-6 py-3 text-left text-xs font-medium text-gray-500 uppercase tracking-wider">Description</th>
                    </tr>
                </thead>
                <tbody class="divide-y divide-gray-200">
                    <tr>
                        <td class="px-6 py-4 whitespace-nowrap text-sm font-medium text-gray-900">temperature</td>
                        <td class="px-6 py-4 whitespace-nowrap text-sm text-gray-500">✅</td>
                        <td class="px-6 py-4 whitespace-nowrap text-sm text-gray-500">✅</td>
                        <td class="px-6 py-4 whitespace-nowrap text-sm text-gray-500">✅</td>
                        <td class="px-6 py-4 text-sm text-gray-500">Controls randomness (0.0-1.0)</td>
                    </tr>
                    <tr class="bg-gray-50">
                        <td class="px-6 py-4 whitespace-nowrap text-sm font-medium text-gray-900">max_tokens</td>
                        <td class="px-6 py-4 whitespace-nowrap text-sm text-gray-500">✅</td>
                        <td class="px-6 py-4 whitespace-nowrap text-sm text-gray-500">✅</td>
                        <td class="px-6 py-4 whitespace-nowrap text-sm text-gray-500">num_predict</td>
                        <td class="px-6 py-4 text-sm text-gray-500">Maximum tokens to generate</td>
                    </tr>
                    <tr>
                        <td class="px-6 py-4 whitespace-nowrap text-sm font-medium text-gray-900">top_p</td>
                        <td class="px-6 py-4 whitespace-nowrap text-sm text-gray-500">✅</td>
                        <td class="px-6 py-4 whitespace-nowrap text-sm text-gray-500">✅</td>
                        <td class="px-6 py-4 whitespace-nowrap text-sm text-gray-500">top_p</td>
                        <td class="px-6 py-4 text-sm text-gray-500">Nucleus sampling parameter</td>
                    </tr>
                    <tr class="bg-gray-50">
                        <td class="px-6 py-4 whitespace-nowrap text-sm font-medium text-gray-900">api_key</td>
                        <td class="px-6 py-4 whitespace-nowrap text-sm text-gray-500">✅</td>
                        <td class="px-6 py-4 whitespace-nowrap text-sm text-gray-500">✅</td>
                        <td class="px-6 py-4 whitespace-nowrap text-sm text-gray-500">❌</td>
                        <td class="px-6 py-4 text-sm text-gray-500">API authentication key</td>
                    </tr>
                </tbody>
            </table>
        </div>
    </div>

    <!-- Using Models in Graphs ---->
    <div class="mb-8">
        <h2 class="text-2xl font-semibold text-gray-900 mb-4">
            <i class="fas fa-project-diagram text-indigo-600 mr-2"></i>
            Using Models in Graphs
        </h2>
        <p class="text-gray-600 mb-4">Configure specific models for graphs and individual nodes:</p>

        <!-- Graph-Level Model Configuration ---->
        <div class="mb-6">
            <h3 class="text-lg font-semibold text-gray-900 mb-3">Graph-Level Model Configuration</h3>
            <div class="bg-gray-50 rounded-lg p-4 border relative group">
                <button class="absolute top-2 right-2 p-2 text-gray-500 hover:text-gray-700 opacity-0 group-hover:opacity-100 transition-opacity" onclick="copyToClipboard('# Create a graph with specific model configuration\nresult = mag.create_graph(\n    name=\"analysis_workflow\",\n    model_config={\n        \"default_model\": \"claude-3-5-sonnet\",  # Default for all nodes\n        \"fallback_model\": \"gpt4-turbo\",        # Fallback if primary fails\n        \"node_specific\": {                      # Node-specific models\n            \"data_processor\": \"llama3-local\",   # Use local model for processing\n            \"final_analysis\": \"claude-3-5-sonnet\" # Use Claude for final analysis\n        }\n    },\n    nodes=[\n        {\n            \"name\": \"data_processor\",\n            \"prompt\": \"Process and clean the input data: {input}\",\n            \"next_node\": \"analyzer\"\n        },\n        {\n            \"name\": \"analyzer\", \n            \"prompt\": \"Analyze the processed data and extract insights\",\n            \"next_node\": \"final_analysis\"\n        },\n        {\n            \"name\": \"final_analysis\",\n            \"prompt\": \"Create a comprehensive final analysis report\"\n        }\n    ]\n)')">
                    <i class="fas fa-copy"></i>
                </button>
                <pre><code class="language-python"># Create a graph with specific model configuration
result = mag.create_graph(
    name="analysis_workflow",
    model_config={
        "default_model": "claude-3-5-sonnet",  # Default for all nodes
        "fallback_model": "gpt4-turbo",        # Fallback if primary fails
        "node_specific": {                      # Node-specific models
            "data_processor": "llama3-local",   # Use local model for processing
            "final_analysis": "claude-3-5-sonnet" # Use Claude for final analysis
        }
    },
    nodes=[
        {
            "name": "data_processor",
            "prompt": "Process and clean the input data: {input}",
            "next_node": "analyzer"
        },
        {
            "name": "analyzer",
            "prompt": "Analyze the processed data and extract insights",
            "next_node": "final_analysis"
        },
        {
            "name": "final_analysis",
            "prompt": "Create a comprehensive final analysis report"
        }
    ]
)</code></pre>
            </div>
        </div>

        <!-- Runtime Model Selection ---->
        <div class="mb-6">
            <h3 class="text-lg font-semibold text-gray-900 mb-3">Runtime Model Selection</h3>
            <div class="bg-gray-50 rounded-lg p-4 border relative group">
                <button class="absolute top-2 right-2 p-2 text-gray-500 hover:text-gray-700 opacity-0 group-hover:opacity-100 transition-opacity" onclick="copyToClipboard('# Override model at runtime\nresult = mag.run_graph(\n    name=\"analysis_workflow\",\n    input_text=\"Analyze this dataset...\",\n    model_override={\n        \"default_model\": \"gpt4-turbo\",  # Override the default\n        \"node_specific\": {\n            \"data_processor\": \"claude-haiku\"  # Use faster model for processing\n        }\n    }\n)\n\n# Use a specific model for the entire graph\nfast_result = mag.run_graph(\n    name=\"simple_task\",\n    input_text=\"Quick analysis needed\",\n    model=\"claude-haiku\"  # Use specific model for all nodes\n)')">
                    <i class="fas fa-copy"></i>
                </button>
                <pre><code class="language-python"># Override model at runtime
result = mag.run_graph(
    name="analysis_workflow",
    input_text="Analyze this dataset...",
    model_override={
        "default_model": "gpt4-turbo",  # Override the default
        "node_specific": {
            "data_processor": "claude-haiku"  # Use faster model for processing
        }
    }
)

# Use a specific model for the entire graph
fast_result = mag.run_graph(
    name="simple_task",
    input_text="Quick analysis needed",
    model="claude-haiku"  # Use specific model for all nodes
)</code></pre>
            </div>
        </div>
    </div>

    <!-- Model Performance and Monitoring ---->
    <div class="mb-8">
        <h2 class="text-2xl font-semibold text-gray-900 mb-4">
            <i class="fas fa-chart-line text-green-600 mr-2"></i>
            Model Performance and Monitoring
        </h2>
        <p class="text-gray-600 mb-4">Monitor model usage, performance, and costs:</p>

        <div class="bg-gray-50 rounded-lg p-4 border relative group mb-6">
            <button class="absolute top-2 right-2 p-2 text-gray-500 hover:text-gray-700 opacity-0 group-hover:opacity-100 transition-opacity" onclick="copyToClipboard('# Get model usage statistics\nstats = mag.get_model_stats()\n\nprint(\"Model Usage Statistics:\")\nprint(\"=\" * 30)\n\nfor model_name, model_stats in stats.items():\n    print(f\"\\n{model_name}:\")\n    print(f\"  Total calls: {model_stats.get(\\'total_calls\\', 0)}\") \n    print(f\"  Success rate: {model_stats.get(\\'success_rate\\', 0):.1%}\") \n    print(f\"  Avg response time: {model_stats.get(\\'avg_response_time\\', 0):.2f}s\") \n    print(f\"  Total tokens: {model_stats.get(\\'total_tokens\\', 0)}\") \n    \n    if model_stats.get(\"total_cost\"):\n        print(f\"  Estimated cost: ${model_stats.get(\\'total_cost\\'):.4f}\") \n\n# Get specific model performance\nmodel_perf = mag.get_model_performance(\"claude-3-5-sonnet\")\nprint(f\"\\nClaude 3.5 Sonnet Performance:\")\nprint(f\"Average latency: {model_perf.get(\\'avg_latency\\'):.2f}ms\") \nprint(f\"Error rate: {model_perf.get(\\'error_rate\\'):.2%}\") ')">
                <i class="fas fa-copy"></i>
            </button>
            <pre><code class="language-python"># Get model usage statistics
stats = mag.get_model_stats()

print("Model Usage Statistics:")
print("=" * 30)

for model_name, model_stats in stats.items():
    print(f"\n{model_name}:")
    print(f"  Total calls: {model_stats.get('total_calls', 0)}")
    print(f"  Success rate: {model_stats.get('success_rate', 0):.1%}")
    print(f"  Avg response time: {model_stats.get('avg_response_time', 0):.2f}s")
    print(f"  Total tokens: {model_stats.get('total_tokens', 0)}")

    if model_stats.get("total_cost"):
        print(f"  Estimated cost: ${model_stats.get('total_cost'):.4f}")

# Get specific model performance
model_perf = mag.get_model_performance("claude-3-5-sonnet")
print(f"\nClaude 3.5 Sonnet Performance:")
print(f"Average latency: {model_perf.get('avg_latency'):.2f}ms")
print(f"Error rate: {model_perf.get('error_rate'):.2%}")</code></pre>
        </div>
    </div>

    <!-- Model Selection Guidelines ---->
    <div class="mb-8">
        <h2 class="text-2xl font-semibold text-gray-900 mb-4">Model Selection Guidelines</h2>

        <div class="grid grid-cols-1 lg:grid-cols-2 gap-6 mb-6">
            <!-- Task-Based Selection ---->
            <div class="border border-gray-200 rounded-lg p-4">
                <h3 class="font-semibold text-gray-900 mb-3">
                    <i class="fas fa-tasks text-blue-500 mr-2"></i>
                    Task-Based Selection
                </h3>
                <div class="space-y-3">
                    <div class="bg-blue-50 rounded p-3">
                        <div class="font-medium text-sm text-blue-900 mb-1">Complex Reasoning</div>
                        <div class="text-xs text-blue-700">GPT-4, Claude 3.5 Sonnet</div>
                    </div>
                    <div class="bg-green-50 rounded p-3">
                        <div class="font-medium text-sm text-green-900 mb-1">Fast Responses</div>
                        <div class="text-xs text-green-700">Claude Haiku, GPT-3.5</div>
                    </div>
                    <div class="bg-purple-50 rounded p-3">
                        <div class="font-medium text-sm text-purple-900 mb-1">Code Generation</div>
                        <div class="text-xs text-purple-700">CodeLlama, GPT-4</div>
                    </div>
                    <div class="bg-orange-50 rounded p-3">
                        <div class="font-medium text-sm text-orange-900 mb-1">Privacy/Local</div>
                        <div class="text-xs text-orange-700">Llama 3, Mistral (via Ollama)</div>
                    </div>
                </div>
            </div>

            <!-- Cost Considerations ---->
            <div class="border border-gray-200 rounded-lg p-4">
                <h3 class="font-semibold text-gray-900 mb-3">
                    <i class="fas fa-dollar-sign text-green-500 mr-2"></i>
                    Cost Considerations
                </h3>
                <div class="space-y-3">
                    <div class="bg-red-50 rounded p-3">
                        <div class="font-medium text-sm text-red-900 mb-1">Premium</div>
                        <div class="text-xs text-red-700">GPT-4, Claude 3.5 Sonnet</div>
                    </div>
                    <div class="bg-yellow-50 rounded p-3">
                        <div class="font-medium text-sm text-yellow-900 mb-1">Moderate</div>
                        <div class="text-xs text-yellow-700">GPT-3.5, Claude Haiku</div>
                    </div>
                    <div class="bg-green-50 rounded p-3">
                        <div class="font-medium text-sm text-green-900 mb-1">Free/Low Cost</div>
                        <div class="text-xs text-green-700">Local models (Ollama)</div>
                    </div>
                </div>
            </div>
        </div>
    </div>

    <!-- Advanced Model Management ---->
    <div class="mb-8">
        <h2 class="text-2xl font-semibold text-gray-900 mb-4">
            <i class="fas fa-cogs text-orange-600 mr-2"></i>
            Advanced Model Management
        </h2>

        <!-- Model Updates ---->
        <div class="mb-6">
            <h3 class="text-lg font-semibold text-gray-900 mb-3">Updating Model Configuration</h3>
            <div class="bg-gray-50 rounded-lg p-4 border relative group">
                <button class="absolute top-2 right-2 p-2 text-gray-500 hover:text-gray-700 opacity-0 group-hover:opacity-100 transition-opacity" onclick="copyToClipboard('# Update an existing model configuration\nresult = mag.update_model(\n    name=\"gpt4-turbo\",\n    config_updates={\n        \"temperature\": 0.5,  # Lower temperature\n        \"max_tokens\": 8000,  # Increase token limit\n        \"top_p\": 0.95\n    }\n)\n\n# Update model status\nmag.activate_model(\"claude-haiku\")    # Activate a model\nmag.deactivate_model(\"old-model\")     # Deactivate a model\n\n# Remove a model entirely\nremove_result = mag.remove_model(\"unused-model\")\nif remove_result.get(\"success\"):\n    print(\"Model removed successfully\")')">
                    <i class="fas fa-copy"></i>
                </button>
                <pre><code class="language-python"># Update an existing model configuration
result = mag.update_model(
    name="gpt4-turbo",
    config_updates={
        "temperature": 0.5,  # Lower temperature
        "max_tokens": 8000,  # Increase token limit
        "top_p": 0.95
    }
)

# Update model status
mag.activate_model("claude-haiku")    # Activate a model
mag.deactivate_model("old-model")     # Deactivate a model

# Remove a model entirely
remove_result = mag.remove_model("unused-model")
if remove_result.get("success"):
    print("Model removed successfully")</code></pre>
            </div>
        </div>

        <!-- Model Validation ---->
        <div class="mb-6">
            <h3 class="text-lg font-semibold text-gray-900 mb-3">Model Validation and Testing</h3>
            <div class="bg-gray-50 rounded-lg p-4 border relative group">
                <button class="absolute top-2 right-2 p-2 text-gray-500 hover:text-gray-700 opacity-0 group-hover:opacity-100 transition-opacity" onclick="copyToClipboard('# Test model connectivity and performance\ntest_result = mag.test_model(\n    model_name=\"claude-3-5-sonnet\",\n    test_prompt=\"Hello, please respond with a simple greeting.\"\n)\n\nif test_result.get(\"success\"):\n    print(f\"Model test successful!\")\n    print(f\"Response time: {test_result.get(\\'response_time\\'):.2f}s\") \n    print(f\"Response: {test_result.get(\\'response\\')}\") \nelse:\n    print(f\"Model test failed: {test_result.get(\\'error\\')}\") \n\n# Validate all models\nvalidation_results = mag.validate_all_models()\nfor model_name, result in validation_results.items():\n    status = \"✅\" if result.get(\"valid\") else \"❌\"\n    print(f\"{status} {model_name}: {result.get(\\'message\\')}\") ')">
                    <i class="fas fa-copy"></i>
                </button>
                <pre><code class="language-python"># Test model connectivity and performance
test_result = mag.test_model(
    model_name="claude-3-5-sonnet",
    test_prompt="Hello, please respond with a simple greeting."
)

if test_result.get("success"):
    print(f"Model test successful!")
    print(f"Response time: {test_result.get('response_time'):.2f}s")
    print(f"Response: {test_result.get('response')}")
else:
    print(f"Model test failed: {test_result.get('error')}")

# Validate all models
validation_results = mag.validate_all_models()
for model_name, result in validation_results.items():
    status = "✅" if result.get("valid") else "❌"
    print(f"{status} {model_name}: {result.get('message')}")</code></pre>
            </div>
        </div>
    </div>

    <!-- Best Practices ---->
    <div class="mb-8">
        <h2 class="text-2xl font-semibold text-gray-900 mb-4">Best Practices</h2>

        <div class="space-y-4">
            <!-- Practice 1 ---->
            <div class="border border-green-200 rounded-lg p-4 bg-green-50">
                <h4 class="font-semibold text-gray-900 mb-2">
                    <i class="fas fa-key text-green-500 mr-2"></i>
                    Environment Variables for API Keys
                </h4>
                <p class="text-gray-700 text-sm mb-2">Never hardcode API keys in your configuration:</p>
                <div class="bg-white rounded p-2 text-xs font-mono border">
                    # Good: Use environment variables<br>
                    config={"api_key": os.getenv("OPENAI_API_KEY")}<br><br>
                    # Bad: Hardcoded keys<br>
                    config={"api_key": "sk-xxxxx"}  # Don't do this!
                </div>
            </div>

            <!-- Practice 2 ---->
            <div class="border border-blue-200 rounded-lg p-4 bg-blue-50">
                <h4 class="font-semibold text-gray-900 mb-2">
                    <i class="fas fa-layer-group text-blue-500 mr-2"></i>
                    Use Fallback Models
                </h4>
                <p class="text-gray-700 text-sm mb-2">Configure fallback models to handle failures:</p>
                <div class="bg-white rounded p-2 text-xs font-mono border">
                    model_config = {<br>
                    &nbsp;&nbsp;&nbsp;&nbsp;"default_model": "claude-3-5-sonnet",<br>
                    &nbsp;&nbsp;&nbsp;&nbsp;"fallback_model": "gpt4-turbo"<br>
                    }
                </div>
            </div>

            <!-- Practice 3 ---->
            <div class="border border-purple-200 rounded-lg p-4 bg-purple-50">
                <h4 class="font-semibold text-gray-900 mb-2">
                    <i class="fas fa-balance-scale text-purple-500 mr-2"></i>
                    Balance Cost and Performance
                </h4>
                <p class="text-gray-700 text-sm mb-2">Choose appropriate models based on task complexity:</p>
                <div class="bg-white rounded p-2 text-xs">
                    • Use fast, cheap models for simple tasks<br>
                    • Reserve premium models for complex reasoning<br>
                    • Consider local models for privacy/cost control
                </div>
            </div>
        </div>
    </div>

    <!-- Troubleshooting ---->
    <div class="bg-gradient-to-r from-yellow-50 to-orange-50 rounded-lg p-6 border border-yellow-200">
        <h3 class="text-lg font-semibold text-gray-900 mb-3">
            <i class="fas fa-wrench text-yellow-600 mr-2"></i>
            Troubleshooting Model Issues
        </h3>
        <p class="text-gray-700 mb-4">Common problems and solutions when working with models:</p>

        <div class="grid grid-cols-1 md:grid-cols-2 gap-4">
            <div class="bg-white rounded-lg p-4 border border-yellow-100">
                <h4 class="font-medium text-gray-900 mb-2">
                    <i class="fas fa-exclamation-triangle text-red-500 mr-2"></i>
                    API Connection Errors
                </h4>
                <ul class="text-sm text-gray-600 space-y-1">
                    <li>• Check API key validity and permissions</li>
                    <li>• Verify network connectivity</li>
                    <li>• Check rate limits and quotas</li>
                    <li>• Validate endpoint URLs</li>
                </ul>
            </div>

            <div class="bg-white rounded-lg p-4 border border-yellow-100">
                <h4 class="font-medium text-gray-900 mb-2">
                    <i class="fas fa-clock text-blue-500 mr-2"></i>
                    Performance Issues
                </h4>
                <ul class="text-sm text-gray-600 space-y-1">
                    <li>• Reduce max_tokens for faster responses</li>
                    <li>• Lower temperature for more predictable output</li>
                    <li>• Use streaming for long responses</li>
                    <li>• Consider model switching for speed</li>
                </ul>
            </div>
        </div>
    </div>
</div>