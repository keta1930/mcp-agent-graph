<div class="max-w-4xl">
    <div class="mb-8">
        <h1 class="text-3xl font-bold text-gray-900 mb-4">Advanced Features</h1>
        <p class="text-lg text-gray-600">Master advanced MAG SDK capabilities including concurrent execution, streaming optimization, custom workflows, and enterprise-grade features for production deployments.</p>
    </div>

    <!-- Concurrent Graph Execution ---->
    <div class="mb-8">
        <h2 class="text-2xl font-semibold text-gray-900 mb-4">
            <i class="fas fa-rocket text-purple-600 mr-2"></i>
            Concurrent Graph Execution
        </h2>
        <p class="text-gray-600 mb-4">Execute multiple graphs simultaneously to maximize throughput and efficiency:</p>

        <div class="bg-gray-50 rounded-lg p-4 border relative group mb-6">
            <button class="absolute top-2 right-2 p-2 text-gray-500 hover:text-gray-700 opacity-0 group-hover:opacity-100 transition-opacity" onclick="copyToClipboard('import asyncio\nfrom concurrent.futures import ThreadPoolExecutor\n\n# Method 1: Concurrent background execution\ndef run_multiple_graphs_async():\n    tasks = [\n        {\"graph\": \"data_processor\", \"input\": \"dataset1.csv\"},\n        {\"graph\": \"sentiment_analyzer\", \"input\": \"reviews.txt\"},\n        {\"graph\": \"report_generator\", \"input\": \"metrics.json\"}\n    ]\n    \n    # Start all graphs in background mode\n    conversation_ids = []\n    for task in tasks:\n        conv_id = mag.run_graph(\n            name=task[\"graph\"],\n            input_text=task[\"input\"],\n            background=True\n        )\n        conversation_ids.append(conv_id)\n        print(f\"Started {task[\\'graph\\']}: {conv_id}\") \n    \n    return conversation_ids\n\n# Method 2: Thread-based parallel execution\ndef execute_graph_task(task):\n    return mag.run_graph(\n        name=task[\"graph\"],\n        input_text=task[\"input\"],\n        stream=False,\n        background=False\n    )\n\ndef run_graphs_in_parallel():\n    tasks = [\n        {\"graph\": \"quick_analysis\", \"input\": \"data1\"},\n        {\"graph\": \"quick_analysis\", \"input\": \"data2\"},\n        {\"graph\": \"quick_analysis\", \"input\": \"data3\"}\n    ]\n    \n    with ThreadPoolExecutor(max_workers=3) as executor:\n        results = list(executor.map(execute_graph_task, tasks))\n    \n    return results')">
                <i class="fas fa-copy"></i>
            </button>
            <pre><code class="language-python">import asyncio
from concurrent.futures import ThreadPoolExecutor

# Method 1: Concurrent background execution
def run_multiple_graphs_async():
    tasks = [
        {"graph": "data_processor", "input": "dataset1.csv"},
        {"graph": "sentiment_analyzer", "input": "reviews.txt"},
        {"graph": "report_generator", "input": "metrics.json"}
    ]

    # Start all graphs in background mode
    conversation_ids = []
    for task in tasks:
        conv_id = mag.run_graph(
            name=task["graph"],
            input_text=task["input"],
            background=True
        )
        conversation_ids.append(conv_id)
        print(f"Started {task['graph']}: {conv_id}")

    return conversation_ids

# Method 2: Thread-based parallel execution
def execute_graph_task(task):
    return mag.run_graph(
        name=task["graph"],
        input_text=task["input"],
        stream=False,
        background=False
    )

def run_graphs_in_parallel():
    tasks = [
        {"graph": "quick_analysis", "input": "data1"},
        {"graph": "quick_analysis", "input": "data2"},
        {"graph": "quick_analysis", "input": "data3"}
    ]

    with ThreadPoolExecutor(max_workers=3) as executor:
        results = list(executor.map(execute_graph_task, tasks))

    return results</code></pre>
        </div>
    </div>

    <!-- Advanced Streaming ---->
    <div class="mb-8">
        <h2 class="text-2xl font-semibont text-gray-900 mb-4">
            <i class="fas fa-stream text-blue-600 mr-2"></i>
            Advanced Streaming Techniques
        </h2>
        <p class="text-gray-600 mb-4">Optimize streaming for real-time applications and user interfaces:</p>

        <!-- Custom Stream Processing ---->
        <div class="mb-6">
            <h3 class="text-lg font-semibold text-gray-900 mb-3">Custom Stream Processing</h3>
            <div class="bg-gray-50 rounded-lg p-4 border relative group">
                <button class="absolute top-2 right-2 p-2 text-gray-500 hover:text-gray-700 opacity-0 group-hover:opacity-100 transition-opacity" onclick="copyToClipboard('class StreamProcessor:\n    def __init__(self):\n        self.current_node = None\n        self.accumulated_content = \"\"\n        self.node_outputs = {}\n    \n    def process_stream(self, graph_name, input_text):\n        stream = mag.run_graph(\n            name=graph_name,\n            input_text=input_text,\n            stream=True\n        )\n        \n        for chunk in stream:\n            chunk_type = chunk.get(\"type\")\n            \n            if chunk_type == \"conversation_created\":\n                conv_id = chunk.get(\"conversation_id\")\n                print(f\"ðŸ”„ Started: {conv_id}\")\n                \n            elif chunk_type == \"node_start\":\n                self.current_node = chunk.get(\"node_name\")\n                self.accumulated_content = \"\"\n                print(f\"\\nðŸŽ¯ Node: {self.current_node}\")\n                \n            elif chunk.get(\"choices\"):\n                # Process AI response chunks\n                delta = chunk[\"choices\"][0].get(\"delta\", {})\n                content = delta.get(\"content\", \"\")\n                \n                if content:\n                    self.accumulated_content += content\n                    # Real-time processing of content\n                    self._process_content_chunk(content)\n                    \n            elif chunk_type == \"node_complete\":\n                # Save complete node output\n                if self.current_node:\n                    self.node_outputs[self.current_node] = self.accumulated_content\n                    self._on_node_complete(self.current_node, self.accumulated_content)\n                    \n            elif chunk_type == \"graph_complete\":\n                final_result = chunk.get(\"final_result\")\n                print(f\"\\nâœ… Complete: {final_result}\")\n                return self.node_outputs\n    \n    def _process_content_chunk(self, content):\n        # Custom processing for each content chunk\n        # Example: Real-time keyword extraction\n        keywords = self._extract_keywords(content)\n        if keywords:\n            print(f\"Keywords: {keywords}\", end=\" \")\n    \n    def _on_node_complete(self, node_name, content):\n        # Custom processing when a node completes\n        word_count = len(content.split())\n        print(f\"\\nðŸ“Š {node_name} completed: {word_count} words\")\n    \n    def _extract_keywords(self, text):\n        # Simple keyword extraction (replace with your logic)\n        import re\n        words = re.findall(r\\'\\b[A-Z][a-z]+\\b\\', text)\n        return words[:3]  # Return first 3 capitalized words\n\n# Usage\nprocessor = StreamProcessor()\nresults = processor.process_stream(\"analysis_graph\", \"Analyze market trends\")')">
                    <i class="fas fa-copy"></i>
                </button>
                <pre><code class="language-python">class StreamProcessor:
    def __init__(self):
        self.current_node = None
        self.accumulated_content = ""
        self.node_outputs = {}

    def process_stream(self, graph_name, input_text):
        stream = mag.run_graph(
            name=graph_name,
            input_text=input_text,
            stream=True
        )

        for chunk in stream:
            chunk_type = chunk.get("type")

            if chunk_type == "conversation_created":
                conv_id = chunk.get("conversation_id")
                print(f"ðŸ”„ Started: {conv_id}")

            elif chunk_type == "node_start":
                self.current_node = chunk.get("node_name")
                self.accumulated_content = ""
                print(f"\nðŸŽ¯ Node: {self.current_node}")

            elif chunk.get("choices"):
                # Process AI response chunks
                delta = chunk["choices"][0].get("delta", {})
                content = delta.get("content", "")

                if content:
                    self.accumulated_content += content
                    # Real-time processing of content
                    self._process_content_chunk(content)

            elif chunk_type == "node_complete":
                # Save complete node output
                if self.current_node:
                    self.node_outputs[self.current_node] = self.accumulated_content
                    self._on_node_complete(self.current_node, self.accumulated_content)

            elif chunk_type == "graph_complete":
                final_result = chunk.get("final_result")
                print(f"\nâœ… Complete: {final_result}")
                return self.node_outputs

    def _process_content_chunk(self, content):
        # Custom processing for each content chunk
        # Example: Real-time keyword extraction
        keywords = self._extract_keywords(content)
        if keywords:
            print(f"Keywords: {keywords}", end=" ")

    def _on_node_complete(self, node_name, content):
        # Custom processing when a node completes
        word_count = len(content.split())
        print(f"\nðŸ“Š {node_name} completed: {word_count} words")

    def _extract_keywords(self, text):
        # Simple keyword extraction (replace with your logic)
        import re
        words = re.findall(r'\b[A-Z][a-z]+\b', text)
        return words[:3]  # Return first 3 capitalized words

# Usage
processor = StreamProcessor()
results = processor.process_stream("analysis_graph", "Analyze market trends")</code></pre>
            </div>
        </div>
    </div>

    <!-- Graph Composition and Nesting ---->
    <div class="mb-8">
        <h2 class="text-2xl font-semibold text-gray-900 mb-4">
            <i class="fas fa-sitemap text-green-600 mr-2"></i>
            Graph Composition and Nesting
        </h2>
        <p class="text-gray-600 mb-4">Build complex workflows by composing and nesting graphs:</p>

        <div class="bg-gray-50 rounded-lg p-4 border relative group mb-6">
            <button class="absolute top-2 right-2 p-2 text-gray-500 hover:text-gray-700 opacity-0 group-hover:opacity-100 transition-opacity" onclick="copyToClipboard('# Create a master workflow that orchestrates multiple sub-graphs\nresult = mag.create_graph(\n    name=\"master_workflow\",\n    nodes=[\n        {\n            \"name\": \"data_ingestion\",\n            \"prompt\": \"Coordinate data ingestion by running the \\'data_processor\\' graph with input: {input}\",\n            \"next_node\": \"analysis_coordinator\",\n            \"sub_graph\": \"data_processor\"  # Reference to sub-graph\n        },\n        {\n            \"name\": \"analysis_coordinator\",\n            \"prompt\": \"Run parallel analysis using multiple specialized graphs based on the processed data\",\n            \"next_node\": \"report_synthesis\",\n            \"parallel_sub_graphs\": [  # Execute multiple graphs in parallel\n                \"sentiment_analysis\",\n                \"trend_analysis\", \n                \"risk_assessment\"\n            ]\n        },\n        {\n            \"name\": \"report_synthesis\",\n            \"prompt\": \"Synthesize all analysis results into a comprehensive final report\",\n            \"sub_graph\": \"report_generator\"\n        }\n    ]\n)\n\n# Advanced graph composition with conditional execution\ndef create_adaptive_workflow():\n    return mag.create_graph(\n        name=\"adaptive_processor\",\n        nodes=[\n            {\n                \"name\": \"input_classifier\",\n                \"prompt\": \"Classify the input type and determine the appropriate processing strategy\",\n                \"next_node\": \"conditional_router\"\n            },\n            {\n                \"name\": \"conditional_router\",\n                \"prompt\": \"Route to appropriate sub-graph based on classification\",\n                \"conditional_routing\": {\n                    \"text_data\": \"text_processing_graph\",\n                    \"image_data\": \"image_processing_graph\",\n                    \"structured_data\": \"data_analysis_graph\",\n                    \"default\": \"generic_processing_graph\"\n                }\n            }\n        ]\n    )')">
                <i class="fas fa-copy"></i>
            </button>
            <pre><code class="language-python"># Create a master workflow that orchestrates multiple sub-graphs
result = mag.create_graph(
    name="master_workflow",
    nodes=[
        {
            "name": "data_ingestion",
            "prompt": "Coordinate data ingestion by running the 'data_processor' graph with input: {input}",
            "next_node": "analysis_coordinator",
            "sub_graph": "data_processor"  # Reference to sub-graph
        },
        {
            "name": "analysis_coordinator",
            "prompt": "Run parallel analysis using multiple specialized graphs based on the processed data",
            "next_node": "report_synthesis",
            "parallel_sub_graphs": [  # Execute multiple graphs in parallel
                "sentiment_analysis",
                "trend_analysis",
                "risk_assessment"
            ]
        },
        {
            "name": "report_synthesis",
            "prompt": "Synthesize all analysis results into a comprehensive final report",
            "sub_graph": "report_generator"
        }
    ]
)

# Advanced graph composition with conditional execution
def create_adaptive_workflow():
    return mag.create_graph(
        name="adaptive_processor",
        nodes=[
            {
                "name": "input_classifier",
                "prompt": "Classify the input type and determine the appropriate processing strategy",
                "next_node": "conditional_router"
            },
            {
                "name": "conditional_router",
                "prompt": "Route to appropriate sub-graph based on classification",
                "conditional_routing": {
                    "text_data": "text_processing_graph",
                    "image_data": "image_processing_graph",
                    "structured_data": "data_analysis_graph",
                    "default": "generic_processing_graph"
                }
            }
        ]
    )</code></pre>
        </div>
    </div>

    <!-- Error Handling and Recovery ---->
    <div class="mb-8">
        <h2 class="text-2xl font-semibold text-gray-900 mb-4">
            <i class="fas fa-shield-alt text-orange-600 mr-2"></i>
            Error Handling and Recovery
        </h2>
        <p class="text-gray-600 mb-4">Implement robust error handling and recovery mechanisms:</p>

        <div class="bg-gray-50 rounded-lg p-4 border relative group mb-6">
            <button class="absolute top-2 right-2 p-2 text-gray-500 hover:text-gray-700 opacity-0 group-hover:opacity-100 transition-opacity" onclick="copyToClipboard('import time\nimport logging\n\nclass RobustGraphExecutor:\n    def __init__(self, max_retries=3, retry_delay=5):\n        self.max_retries = max_retries\n        self.retry_delay = retry_delay\n        self.logger = logging.getLogger(__name__)\n    \n    def execute_with_recovery(self, graph_name, input_text, **kwargs):\n        \"\"\"Execute graph with automatic retry and error recovery\"\"\"\n        last_error = None\n        \n        for attempt in range(self.max_retries + 1):\n            try:\n                # Attempt graph execution\n                result = mag.run_graph(\n                    name=graph_name,\n                    input_text=input_text,\n                    **kwargs\n                )\n                \n                # Validate result\n                if self._validate_result(result):\n                    return result\n                else:\n                    raise ValueError(\"Invalid result format\")\n                    \n            except Exception as e:\n                last_error = e\n                self.logger.warning(f\"Attempt {attempt + 1} failed: {str(e)}\")\n                \n                if attempt < self.max_retries:\n                    # Try recovery strategies\n                    recovery_success = self._attempt_recovery(graph_name, e)\n                    \n                    if recovery_success:\n                        self.logger.info(\"Recovery successful, retrying...\")\n                    \n                    time.sleep(self.retry_delay * (attempt + 1))  # Exponential backoff\n                else:\n                    # Final attempt failed, raise the last error\n                    self.logger.error(f\"All {self.max_retries + 1} attempts failed\")\n                    raise last_error\n    \n    def _validate_result(self, result):\n        \"\"\"Validate the graph execution result\"\"\"\n        if not result:\n            return False\n            \n        # Check for required fields\n        required_fields = [\"final_result\", \"_id\"]\n        return all(field in result for field in required_fields)\n    \n    def _attempt_recovery(self, graph_name, error):\n        \"\"\"Attempt to recover from specific error types\"\"\"\n        error_type = type(error).__name__\n        \n        if \"ConnectionError\" in error_type:\n            # Network connectivity issue\n            self.logger.info(\"Attempting network recovery...\")\n            return self._check_network_connectivity()\n            \n        elif \"ModelError\" in error_type:\n            # Model-specific error, try fallback\n            self.logger.info(\"Attempting model fallback...\")\n            return self._switch_to_fallback_model(graph_name)\n            \n        elif \"RateLimitError\" in error_type:\n            # Rate limit hit, wait longer\n            self.logger.info(\"Rate limit hit, extending wait time...\")\n            time.sleep(30)  # Wait 30 seconds for rate limit reset\n            return True\n            \n        return False\n    \n    def _check_network_connectivity(self):\n        \"\"\"Check if MAG service is reachable\"\"\"\n        try:\n            # Simple connectivity test\n            models = mag.list_models()\n            return True\n        except:\n            return False\n    \n    def _switch_to_fallback_model(self, graph_name):\n        \"\"\"Switch to a fallback model configuration\"\"\"\n        try:\n            # Get current graph config\n            config = mag.get_graph_config(graph_name)\n            \n            # Modify to use fallback model\n            if \"model_config\" in config:\n                config[\"model_config\"][\"default_model\"] = config[\"model_config\"].get(\n                    \"fallback_model\", \"claude-haiku\"  # Default fallback\n                )\n                \n                # Update graph with fallback config\n                mag.update_graph_config(graph_name, config)\n                return True\n        except:\n            pass\n            \n        return False\n\n# Usage\nexecutor = RobustGraphExecutor(max_retries=3, retry_delay=5)\nresult = executor.execute_with_recovery(\n    \"sensitive_analysis\",\n    \"Process this critical data\",\n    background=False\n)')">
                <i class="fas fa-copy"></i>
            </button>
            <pre><code class="language-python">import time
import logging

class RobustGraphExecutor:
    def __init__(self, max_retries=3, retry_delay=5):
        self.max_retries = max_retries
        self.retry_delay = retry_delay
        self.logger = logging.getLogger(__name__)

    def execute_with_recovery(self, graph_name, input_text, **kwargs):
        """Execute graph with automatic retry and error recovery"""
        last_error = None

        for attempt in range(self.max_retries + 1):
            try:
                # Attempt graph execution
                result = mag.run_graph(
                    name=graph_name,
                    input_text=input_text,
                    **kwargs
                )

                # Validate result
                if self._validate_result(result):
                    return result
                else:
                    raise ValueError("Invalid result format")

            except Exception as e:
                last_error = e
                self.logger.warning(f"Attempt {attempt + 1} failed: {str(e)}")

                if attempt < self.max_retries:
                    # Try recovery strategies
                    recovery_success = self._attempt_recovery(graph_name, e)

                    if recovery_success:
                        self.logger.info("Recovery successful, retrying...")

                    time.sleep(self.retry_delay * (attempt + 1))  # Exponential backoff
                else:
                    # Final attempt failed, raise the last error
                    self.logger.error(f"All {self.max_retries + 1} attempts failed")
                    raise last_error

    def _validate_result(self, result):
        """Validate the graph execution result"""
        if not result:
            return False

        # Check for required fields
        required_fields = ["final_result", "_id"]
        return all(field in result for field in required_fields)

    def _attempt_recovery(self, graph_name, error):
        """Attempt to recover from specific error types"""
        error_type = type(error).__name__

        if "ConnectionError" in error_type:
            # Network connectivity issue
            self.logger.info("Attempting network recovery...")
            return self._check_network_connectivity()

        elif "ModelError" in error_type:
            # Model-specific error, try fallback
            self.logger.info("Attempting model fallback...")
            return self._switch_to_fallback_model(graph_name)

        elif "RateLimitError" in error_type:
            # Rate limit hit, wait longer
            self.logger.info("Rate limit hit, extending wait time...")
            time.sleep(30)  # Wait 30 seconds for rate limit reset
            return True

        return False

    def _check_network_connectivity(self):
        """Check if MAG service is reachable"""
        try:
            # Simple connectivity test
            models = mag.list_models()
            return True
        except:
            return False

    def _switch_to_fallback_model(self, graph_name):
        """Switch to a fallback model configuration"""
        try:
            # Get current graph config
            config = mag.get_graph_config(graph_name)

            # Modify to use fallback model
            if "model_config" in config:
                config["model_config"]["default_model"] = config["model_config"].get(
                    "fallback_model", "claude-haiku"  # Default fallback
                )

                # Update graph with fallback config
                mag.update_graph_config(graph_name, config)
                return True
        except:
            pass

        return False

# Usage
executor = RobustGraphExecutor(max_retries=3, retry_delay=5)
result = executor.execute_with_recovery(
    "sensitive_analysis",
    "Process this critical data",
    background=False
)</code></pre>
        </div>
    </div>

    <!-- Performance Optimization ---->
    <div class="mb-8">
        <h2 class="text-2xl font-semibold text-gray-900 mb-4">
            <i class="fas fa-tachometer-alt text-red-600 mr-2"></i>
            Performance Optimization
        </h2>
        <p class="text-gray-600 mb-4">Advanced techniques for optimizing graph execution performance:</p>

        <div class="grid grid-cols-1 lg:grid-cols-2 gap-6 mb-6">
            <!-- Caching Strategy ---->
            <div class="border border-gray-200 rounded-lg p-4">
                <h3 class="font-semibold text-gray-900 mb-3">
                    <i class="fas fa-memory text-blue-500 mr-2"></i>
                    Intelligent Caching
                </h3>
                <div class="bg-gray-50 rounded p-3 text-sm">
                    <pre><code class="language-python"># Implement result caching
class CachedGraphExecutor:
    def __init__(self):
        self.cache = {}

    def execute_with_cache(self, graph_name, input_text):
        # Create cache key
        cache_key = f"{graph_name}:{hash(input_text)}"

        if cache_key in self.cache:
            print("Cache hit!")
            return self.cache[cache_key]

        # Execute and cache result
        result = mag.run_graph(graph_name, input_text)
        self.cache[cache_key] = result
        return result</code></pre>
                </div>
            </div>

            <!-- Batch Processing ---->
            <div class="border border-gray-200 rounded-lg p-4">
                <h3 class="font-semibold text-gray-900 mb-3">
                    <i class="fas fa-layer-group text-green-500 mr-2"></i>
                    Batch Processing
                </h3>
                <div class="bg-gray-50 rounded p-3 text-sm">
                    <pre><code class="language-python"># Batch multiple inputs
def batch_process(graph_name, inputs, batch_size=5):
    results = []

    for i in range(0, len(inputs), batch_size):
        batch = inputs[i:i+batch_size]
        batch_input = "\n".join([
            f"Item {j+1}: {item}"
            for j, item in enumerate(batch)
        ])

        result = mag.run_graph(graph_name, batch_input)
        results.append(result)

    return results</code></pre>
                </div>
            </div>
        </div>
    </div>

    <!-- Custom Integrations ---->
    <div class="mb-8">
        <h2 class="text-2xl font-semibold text-gray-900 mb-4">
            <i class="fas fa-puzzle-piece text-indigo-600 mr-2"></i>
            Custom Integrations
        </h2>
        <p class="text-gray-600 mb-4">Build custom integrations with external systems and services:</p>

        <div class="bg-gray-50 rounded-lg p-4 border relative group mb-6">
            <button class="absolute top-2 right-2 p-2 text-gray-500 hover:text-gray-700 opacity-0 group-hover:opacity-100 transition-opacity" onclick="copyToClipboard('# Custom webhook integration\nclass WebhookIntegration:\n    def __init__(self, webhook_url):\n        self.webhook_url = webhook_url\n    \n    def execute_with_webhook(self, graph_name, input_text, notify_on=[\"start\", \"complete\", \"error\"]):\n        \"\"\"Execute graph with webhook notifications\"\"\"\n        import requests\n        import json\n        \n        conversation_id = None\n        \n        try:\n            # Notify start\n            if \"start\" in notify_on:\n                self._send_webhook({\n                    \"event\": \"graph_started\",\n                    \"graph_name\": graph_name,\n                    \"input_text\": input_text[:100] + \"...\" if len(input_text) > 100 else input_text\n                })\n            \n            # Execute graph\n            if \"complete\" in notify_on:\n                # Use streaming to get conversation_id early\n                stream = mag.run_graph(graph_name, input_text, stream=True)\n                \n                for chunk in stream:\n                    if chunk.get(\"type\") == \"conversation_created\":\n                        conversation_id = chunk.get(\"conversation_id\")\n                        \n                    elif chunk.get(\"type\") == \"graph_complete\":\n                        # Notify completion\n                        self._send_webhook({\n                            \"event\": \"graph_completed\",\n                            \"conversation_id\": conversation_id,\n                            \"final_result\": chunk.get(\"final_result\")\n                        })\n                        return chunk\n            else:\n                # Simple execution without progress tracking\n                result = mag.run_graph(graph_name, input_text)\n                return result\n                \n        except Exception as e:\n            # Notify error\n            if \"error\" in notify_on:\n                self._send_webhook({\n                    \"event\": \"graph_error\",\n                    \"conversation_id\": conversation_id,\n                    \"error\": str(e)\n                })\n            raise e\n    \n    def _send_webhook(self, payload):\n        \"\"\"Send webhook notification\"\"\"\n        try:\n            import requests\n            response = requests.post(\n                self.webhook_url,\n                json=payload,\n                timeout=10\n            )\n            response.raise_for_status()\n        except Exception as e:\n            print(f\"Webhook failed: {e}\")\n\n# Custom database integration\nclass DatabaseIntegration:\n    def __init__(self, db_connection_string):\n        self.db_connection = db_connection_string\n    \n    def log_execution(self, graph_name, input_text, result):\n        \"\"\"Log graph execution to database\"\"\"\n        # Implementation depends on your database\n        # Example with SQLAlchemy:\n        from sqlalchemy import create_engine, text\n        \n        engine = create_engine(self.db_connection)\n        \n        with engine.connect() as conn:\n            conn.execute(text(\"\"\"\n                INSERT INTO graph_executions \n                (graph_name, input_text, result, timestamp)\n                VALUES (:graph_name, :input_text, :result, NOW())\n            \"\"\"), {\n                \"graph_name\": graph_name,\n                \"input_text\": input_text,\n                \"result\": str(result)\n            })')">
                <i class="fas fa-copy"></i>
            </button>
            <pre><code class="language-python"># Custom webhook integration
class WebhookIntegration:
    def __init__(self, webhook_url):
        self.webhook_url = webhook_url

    def execute_with_webhook(self, graph_name, input_text, notify_on=["start", "complete", "error"]):
        """Execute graph with webhook notifications"""
        import requests
        import json

        conversation_id = None

        try:
            # Notify start
            if "start" in notify_on:
                self._send_webhook({
                    "event": "graph_started",
                    "graph_name": graph_name,
                    "input_text": input_text[:100] + "..." if len(input_text) > 100 else input_text
                })

            # Execute graph
            if "complete" in notify_on:
                # Use streaming to get conversation_id early
                stream = mag.run_graph(graph_name, input_text, stream=True)

                for chunk in stream:
                    if chunk.get("type") == "conversation_created":
                        conversation_id = chunk.get("conversation_id")

                    elif chunk.get("type") == "graph_complete":
                        # Notify completion
                        self._send_webhook({
                            "event": "graph_completed",
                            "conversation_id": conversation_id,
                            "final_result": chunk.get("final_result")
                        })
                        return chunk
            else:
                # Simple execution without progress tracking
                result = mag.run_graph(graph_name, input_text)
                return result

        except Exception as e:
            # Notify error
            if "error" in notify_on:
                self._send_webhook({
                    "event": "graph_error",
                    "conversation_id": conversation_id,
                    "error": str(e)
                })
            raise e

    def _send_webhook(self, payload):
        """Send webhook notification"""
        try:
            import requests
            response = requests.post(
                self.webhook_url,
                json=payload,
                timeout=10
            )
            response.raise_for_status()
        except Exception as e:
            print(f"Webhook failed: {e}")

# Custom database integration
class DatabaseIntegration:
    def __init__(self, db_connection_string):
        self.db_connection = db_connection_string

    def log_execution(self, graph_name, input_text, result):
        """Log graph execution to database"""
        # Implementation depends on your database
        # Example with SQLAlchemy:
        from sqlalchemy import create_engine, text

        engine = create_engine(self.db_connection)

        with engine.connect() as conn:
            conn.execute(text("""
                INSERT INTO graph_executions
                (graph_name, input_text, result, timestamp)
                VALUES (:graph_name, :input_text, :result, NOW())
            """), {
                "graph_name": graph_name,
                "input_text": input_text,
                "result": str(result)
            })</code></pre>
        </div>
    </div>

    <!-- Production Deployment ---->
    <div class="mb-8">
        <h2 class="text-2xl font-semibold text-gray-900 mb-4">
            <i class="fas fa-cloud text-purple-600 mr-2"></i>
            Production Deployment
        </h2>
        <p class="text-gray-600 mb-4">Best practices for deploying MAG SDK in production environments:</p>

        <div class="space-y-4">
            <!-- Environment Configuration ---->
            <div class="border border-blue-200 rounded-lg p-4 bg-blue-50">
                <h4 class="font-semibold text-gray-900 mb-2">
                    <i class="fas fa-cog text-blue-500 mr-2"></i>
                    Environment Configuration
                </h4>
                <div class="bg-white rounded p-2 text-sm">
                    <strong>Environment Variables:</strong><br>
                    â€¢ <code>MAG_ENV=production</code> - Set production mode<br>
                    â€¢ <code>MAG_LOG_LEVEL=INFO</code> - Configure logging<br>
                    â€¢ <code>MAG_MAX_WORKERS=4</code> - Set worker pool size<br>
                    â€¢ <code>MAG_CACHE_SIZE=1000</code> - Configure cache size
                </div>
            </div>

            <!-- Monitoring and Logging ---->
            <div class="border border-green-200 rounded-lg p-4 bg-green-50">
                <h4 class="font-semibold text-gray-900 mb-2">
                    <i class="fas fa-chart-line text-green-500 mr-2"></i>
                    Monitoring and Logging
                </h4>
                <div class="bg-white rounded p-2 text-sm">
                    <strong>Key Metrics to Monitor:</strong><br>
                    â€¢ Graph execution times and success rates<br>
                    â€¢ Model API usage and costs<br>
                    â€¢ Memory usage and performance<br>
                    â€¢ Error rates and failure patterns
                </div>
            </div>

            <!-- Security Considerations ---->
            <div class="border border-red-200 rounded-lg p-4 bg-red-50">
                <h4 class="font-semibold text-gray-900 mb-2">
                    <i class="fas fa-shield-alt text-red-500 mr-2"></i>
                    Security Considerations
                </h4>
                <div class="bg-white rounded p-2 text-sm">
                    <strong>Security Checklist:</strong><br>
                    â€¢ Store API keys in secure environment variables<br>
                    â€¢ Implement input validation and sanitization<br>
                    â€¢ Use HTTPS for all API communications<br>
                    â€¢ Regular security updates and patches
                </div>
            </div>
        </div>
    </div>

    <!-- Expert Tips and Tricks ---->
    <div class="bg-gradient-to-r from-purple-50 to-pink-50 rounded-lg p-6 border border-purple-200">
        <h3 class="text-lg font-semibold text-gray-900 mb-3">
            <i class="fas fa-lightbulb text-purple-600 mr-2"></i>
            Expert Tips and Tricks
        </h3>
        <p class="text-gray-700 mb-4">Advanced techniques used by experienced MAG SDK developers:</p>

        <div class="grid grid-cols-1 md:grid-cols-2 gap-4">
            <div class="bg-white rounded-lg p-4 border border-purple-100">
                <h4 class="font-medium text-gray-900 mb-2">
                    <i class="fas fa-rocket text-blue-500 mr-2"></i>
                    Performance Hacks
                </h4>
                <ul class="text-sm text-gray-600 space-y-1">
                    <li>â€¢ Use background execution for long-running graphs</li>
                    <li>â€¢ Implement intelligent result caching</li>
                    <li>â€¢ Batch similar requests together</li>
                    <li>â€¢ Use local models for simple tasks</li>
                </ul>
            </div>

            <div class="bg-white rounded-lg p-4 border border-purple-100">
                <h4 class="font-medium text-gray-900 mb-2">
                    <i class="fas fa-brain text-green-500 mr-2"></i>
                    Optimization Strategies
                </h4>
                <ul class="text-sm text-gray-600 space-y-1">
                    <li>â€¢ Profile and optimize prompt engineering</li>
                    <li>â€¢ Use model-specific optimizations</li>
                    <li>â€¢ Implement smart retry mechanisms</li>
                    <li>â€¢ Monitor and optimize token usage</li>
                </ul>
            </div>

            <div class="bg-white rounded-lg p-4 border border-purple-100">
                <h4 class="font-medium text-gray-900 mb-2">
                    <i class="fas fa-shield text-orange-500 mr-2"></i>
                    Reliability Patterns
                </h4>
                <ul class="text-sm text-gray-600 space-y-1">
                    <li>â€¢ Always implement fallback mechanisms</li>
                    <li>â€¢ Use circuit breakers for external APIs</li>
                    <li>â€¢ Implement comprehensive error logging</li>
                    <li>â€¢ Design for graceful degradation</li>
                </ul>
            </div>

            <div class="bg-white rounded-lg p-4 border border-purple-100">
                <h4 class="font-medium text-gray-900 mb-2">
                    <i class="fas fa-users text-purple-500 mr-2"></i>
                    Scaling Strategies
                </h4>
                <ul class="text-sm text-gray-600 space-y-1">
                    <li>â€¢ Design stateless graph architectures</li>
                    <li>â€¢ Use horizontal scaling patterns</li>
                    <li>â€¢ Implement proper resource management</li>
                    <li>â€¢ Plan for peak load scenarios</li>
                </ul>
            </div>
        </div>
    </div>
</div>